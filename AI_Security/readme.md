# Generative AI in Cybersecurity
- (1) Threat Detection and Analysis
- (2) Phishing Detection and Respons
- (3) Incident Response
- (4) Security Automation
- (5) Cyber Forensics
- (6) Chatbots
- (7) Penetration Testing
- (8) Security Protocols Verification
- (9) Security Training and Awareness:
  - LLMs can generate training materials tailored to an organization’s needs.
  - They can also simulate phishing attacks and other security scenarios to train employees to recognize and respond to security threats 


## Review
- Generative AI in Cybersecurity: A Comprehensive Review of LLM Applications and Vulnerabilities
  - https://arxiv.org/abs/2405.12750
- Review of Generative AI Methods in Cybersecurity
  - https://arxiv.org/abs/2403.08701v1
- A survey of large language models in cybersecurity
  - https://arxiv.org/abs/2402.16968

## Project
- PentestGPT(2023.8)
- AutoAttacker(2024.3)
- PTHelper(2024.6)
- BreachSeek(2024.9)
- PentestAgent(2024.11)
- HackSynth(2024.12)
- AutoPentest(2025.5)

## Benchmark
- See https://arxiv.org/pdf/2405.12750
  - TABLE I: Summary of Related Reviews on Large Language Models
  - TABLE II: RNN-based models for Cyber Security
  - TABLE III: Transformer-based models for Cyber Security (Part I).
  - TABLE IV: Transformer-based models for Cyber Security (Part II)
  - Fig. 4: LLM-based Solutions for Cyber Security Use Cases.
  - TABLE V: Comparison of Large Language Models
  - TABLE VIII: Comparison of 19 LLMs Models’ Performance in Hardware Security Knowledge
  - TABLE IX: Comparison of 42 LLMs Models’ Performance in Cyber Security Knowledge
  - TABLE XVII: Comparison of Benchmarks for Evaluating LLMs in Cybersecurity Knowledge
  - TABLE XVIII: Optimization Strategies for Large Language Models in Cybersecurity
- AI-Pentest-Benchmark
  - https://github.com/isamu-isozaki/AI-Pentest-Benchmark 
- CyberMetric Dataset
  - https://github.com/CyberMetric
  - CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation for Evaluating LLMs in Cybersecurity Knowledge
    - https://arxiv.org/abs/2402.07688
- Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models
  - https://arxiv.org/abs/2408.08926
  - https://cybench.github.io/ 
- Catastrophic Cyber Capabilities Benchmark (3CB)(2024.10)
  - https://cybercapabilities.org/
  - Catastrophic Cyber Capabilities Benchmark (3CB): Robustly Evaluating LLM Agent Cyber Offense Capabilities
    - https://arxiv.org/abs/2410.09114  
- CyberSecEval 1 


## Test
- Teams of LLM Agents can Exploit Zero-Day Vulnerabilities
  - https://arxiv.org/abs/2406.01637 
- Towards Automated Penetration Testing: Introducing LLM Benchmark, Analysis, and Improvements
  - https://arxiv.org/abs/2410.17141
- From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing
  - https://arxiv.org/abs/2509.14289 
