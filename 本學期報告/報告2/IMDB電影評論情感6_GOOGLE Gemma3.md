## Google Gemma 3 æ¨¡å‹ï¼ˆå¯èƒ½æ‚¨æ˜¯åœ¨ç”¨ç°¡å¯«æŒ‡ä»£ çš„ 4-bit é‡åŒ–ç‰ˆæœ¬ï¼Œæˆ–è€…æ˜¯ â€œGem for Gemmaâ€ çš„æŸç¨®æ˜µç¨±ï¼‰
- Gemma 3 æ˜¯ Google åœ¨ 2025 å¹´ç™¼ä½ˆçš„å¤šæ¨¡æ…‹é–‹æ”¾æ¨¡å‹ï¼Œéå¸¸é©åˆç”¨æ–¼æ–‡æœ¬åˆ†æä»»å‹™ã€‚
- æœ¬æŒ‡å—å°‡å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Gemma 3-4B-ITï¼ˆæŒ‡ä»¤å¾®èª¿ç‰ˆï¼‰å° IMDB é›»å½±è©•è«–æ•¸æ“šé›† é€²è¡Œæƒ…æ„Ÿåˆ†æï¼ˆSentiment Analysisï¼‰ã€‚
- ç‚ºäº†åœ¨å–®å¼µ GPUï¼ˆå¦‚ Colab T4 æˆ–æœ¬åœ°é¡¯å¡ï¼‰ä¸Šé«˜æ•ˆé‹è¡Œï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ 4-bit é‡åŒ–è¼‰å…¥æ¨¡å‹ã€‚
- ç‰ˆæœ¬1:ç²¾ç°¡ç‰ˆ
- ç‰ˆæœ¬2:å¾®èª¿ç‰ˆ
  - å¦‚æœæ‚¨æƒ³è®“ Gemma 3 åœ¨ IMDB ä¸Šé”åˆ°æ¥µè‡´çš„æº–ç¢ºç‡ï¼ˆä¾‹å¦‚ç”¨æ–¼å­¸è¡“ç«¶è³½ï¼‰ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ PEFT å’Œ LoRA æŠ€è¡“é€²è¡Œå¾®èª¿ï¼š
  - 1.	æº–å‚™æ•¸æ“š: å°‡ IMDB è™•ç†æˆ {"text": "...", "label": "..."} æ ¼å¼ã€‚
  - 2.	ä½¿ç”¨ LoRA: å‡çµæ¨¡å‹å¤§éƒ¨åˆ†åƒæ•¸ï¼Œåªè¨“ç·´ Low-Rank Adaptersã€‚
  - 3.	è¨“ç·´åº«: ä½¿ç”¨ Hugging Face çš„ trl åº«ä¸­çš„ SFTTrainerã€‚

## 1. ç’°å¢ƒæº–å‚™
- é¦–å…ˆï¼Œæ‚¨éœ€è¦å®‰è£å¿…è¦çš„ Python åº«ã€‚ Gemma 3 éœ€è¦è¼ƒæ–°çš„ ç‰ˆæœ¬ã€‚transformers
```bash
pip install -U transformers torch accelerate bitsandbytes datasets
```
## 2. å®Œæ•´ä»£ç¢¼ç¯„ä¾‹ï¼šä½¿ç”¨ Gemma 3 åˆ†æ IMDB
- ç¯„ä¾‹ç¨‹å¼æœƒåŸ·è¡Œä»¥ä¸‹æ“ä½œï¼š
  - è¼‰å…¥IMDBæ•¸æ“šé›†ã€‚
  - è¼‰å…¥ 4-bit é‡åŒ–çš„ Gemma 3 æ¨¡å‹ï¼ˆç¯€çœé¡¯å­˜ï¼‰ã€‚
  - æ§‹å»º Prompt è®“æ¨¡å‹åˆ¤æ–·è©•è«–æ˜¯ â€œPositiveâ€ ï¼ˆæ­£é¢ï¼‰ é‚„æ˜¯ â€œNegativeâ€ ï¼ˆè² é¢ï¼‰ã€‚
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from datasets import load_dataset
from tqdm import tqdm

# 1. è¨­ç½®æ¨¡å‹ ID (ä½¿ç”¨ Google çš„å®˜æ–¹ Gemma 3 æŒ‡ä»¤å¾®èª¿ç‰ˆ)
# å¦‚æœæ‚¨çš„é¡¯å­˜è¼ƒå°‘ï¼Œå»ºè­°ä½¿ç”¨ 4B æˆ– 1B ç‰ˆæœ¬
model_id = "google/gemma-3-4b-it"

# 2. é…ç½® 4-bit é‡åŒ– (é€™å°æ‡‰äº†æ‚¨å¯èƒ½æŒ‡çš„ '4gemma' / 4-bit)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)

print("æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.bfloat16
)

# 3. è¼‰å…¥ IMDB è³‡æ–™é›† (åƒ…å–å‰ 5 æ¢ä½œç‚ºæ¼”ç¤º)
print("æ­£åœ¨è¼‰å…¥ IMDB è³‡æ–™...")
dataset = load_dataset("imdb", split="test").shuffle(seed=42).select(range(5))

# 4. å®šç¾©åˆ†æå‡½æ•¸
def analyze_sentiment(review_text):
    # æ§‹å»º Prompt (æç¤ºè©)
    # Gemma 3 å°æŒ‡ä»¤éµå¾ªèƒ½åŠ›å¾ˆå¼·ï¼Œæˆ‘å€‘ç›´æ¥è¦æ±‚å®ƒè¼¸å‡º json æˆ–ç‰¹å®šæ ¼å¼
    prompt = f"""
    Analyze the sentiment of the following movie review. 
    Classify it as either 'Positive' or 'Negative'.
    Provide the output in the format: Sentiment: [Label]

    Review: "{review_text[:1000]}..." 
    
    Answer:
    """
    
    # ä½¿ç”¨èŠå¤©ç¯„æœ¬ (æ¨è–¦ç”¨æ–¼ instruct æ¨¡å‹)
    messages = [
        {"role": "user", "content": prompt}
    ]
    input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to(model.device)
    
    outputs = model.generate(
        input_ids, 
        max_new_tokens=10, # æˆ‘å€‘åªéœ€è¦ç°¡çŸ­çš„å›ç­”
        do_sample=False,   # ä½¿ç”¨è²ªå©ªè§£ç¢¼ä»¥ç²å¾—ç¢ºå®šæ€§çµæœ
        temperature=0.0
    )
    
    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)
    return response.strip()

# 5. é‹è¡Œåˆ†æ
print("-" * 50)
print(f"ä½¿ç”¨æ¨¡å‹: {model_id} é€²è¡Œåˆ†æ")
print("-" * 50)

for i, item in enumerate(dataset):
    review = item['text']
    true_label = "Positive" if item['label'] == 1 else "Negative"
    
    # è®“ Gemma 3 é€²è¡Œé æ¸¬
    prediction = analyze_sentiment(review)
    
    print(f"Review #{i+1}:")
    print(f"åŸæ–‡ç‰‡æ®µ: {review[:100]}...")
    print(f"çœŸå¯¦æ¨™ç±¤: {true_label}")
    print(f"Gemmaé æ¸¬: {prediction}")
    print("-" * 30)
```

# ç‰ˆæœ¬2:å¾®èª¿ç‰ˆ
- ä½¿ç”¨ PEFT (LoRA) å’Œ TRL (SFTTrainer) å° Gemma 3 é€²è¡Œå¾®èª¿çš„ç¨‹å¼ç¢¼ã€‚
- é€™å€‹æ–¹æ³•åªéœ€è¦è¨“ç·´æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†åƒæ•¸ï¼Œæ¥µå¤§åœ°æ¸›å°‘äº†é¡¯å­˜éœ€æ±‚å’Œè¨“ç·´æ™‚é–“ã€‚
- ğŸš€ 1. ç’°å¢ƒèˆ‡å‡½å¼åº«æº–å‚™
- ç¢ºä¿å·²å®‰è£ï¼štransformers, torch, accelerate, bitsandbytes, datasets
- pip install peft trl
- ğŸ’» 2. Gemma 3 Lora å¾®èª¿ç¨‹å¼ç¢¼
```python
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
)
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer # Supervised Fine-Tuning Trainer

# --- åƒæ•¸è¨­å®š ---
# é¸æ“‡ Gemma 3 æŒ‡ä»¤å¾®èª¿ç‰ˆæœ¬
model_id = "google/gemma-3-4b-it" 
dataset_id = "imdb" 
output_dir = "gemma3_imdb_sentiment_lora" # è¨“ç·´çµæœå„²å­˜è·¯å¾‘

# 1. å®šç¾© Prompt æ¨¡æ¿ (å¿…é ˆå°‡è³‡æ–™è½‰ç‚ºæ¨¡å‹èƒ½ç†è§£çš„æ ¼å¼)
def format_imdb_data(example):
    # 'text' æ˜¯é›»å½±è©•è«–ï¼Œ'label' æ˜¯ 0 (è² é¢) æˆ– 1 (æ­£é¢)
    sentiment = "æ­£é¢ (Positive)" if example["label"] == 1 else "è² é¢ (Negative)"
    
    # é€™æ˜¯ SFT çš„æ¨™æº–æ ¼å¼ï¼šç”¨æˆ¶è¼¸å…¥ -> æ¨¡å‹è¼¸å‡º
    text = f"åˆ†æä»¥ä¸‹é›»å½±è©•è«–çš„æƒ…æ„Ÿå‚¾å‘ã€‚è«‹åªå›ç­” 'æ­£é¢ (Positive)' æˆ– 'è² é¢ (Negative)'ã€‚\n\n"
    text += f"è©•è«–: {example['text']}\n\n"
    text += f"æƒ…æ„Ÿå‚¾å‘: {sentiment}"
    return {"text": text}

# 2. è¼‰å…¥è³‡æ–™é›†
print("æ­£åœ¨è¼‰å…¥ IMDB è¨“ç·´è³‡æ–™é›†...")
# æˆ‘å€‘ä½¿ç”¨ train split é€²è¡Œè¨“ç·´ï¼Œä¸¦é™åˆ¶å¤§å°ä»¥é€²è¡Œå¿«é€Ÿæ¼”ç¤º
dataset = load_dataset(dataset_id, split="train[:5000]").map(format_imdb_data) 


# 3. é…ç½® 4-bit é‡åŒ– (Q-LoRA åŸºç¤)
print("æ­£åœ¨é…ç½® 4-bit é‡åŒ–...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4", # Normal Float 4-bit
    bnb_4bit_compute_dtype=torch.bfloat16 # ä½¿ç”¨ bf16 é€²è¡Œè¨ˆç®—ï¼Œæé«˜æ€§èƒ½
)

# 4. è¼‰å…¥æ¨¡å‹å’Œ Tokenizer
print(f"æ­£åœ¨è¼‰å…¥æ¨¡å‹ï¼š{model_id}...")
tokenizer = AutoTokenizer.from_pretrained(model_id)

# ç¢ºä¿æ¨¡å‹çš„ pad_token è¨­ç½®æ­£ç¢º
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token 

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
)

# 5. é…ç½® Q-LoRA
# åœ¨ k-bit è¨“ç·´ä¸­ï¼Œå¿…é ˆå…ˆæº–å‚™æ¨¡å‹
model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

# LoRA é…ç½®ï¼Œé€™æ˜¯ PEFT çš„æ ¸å¿ƒ
lora_config = LoraConfig(
    r=16, # LoRA çš„ç§© (rank)ï¼Œå€¼è¶Šå¤§æ€§èƒ½å¯èƒ½è¶Šå¥½ï¼Œä½†è¨ˆç®—é‡æ›´å¤§
    lora_alpha=16, # LoRA ç¸®æ”¾åƒæ•¸
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM", # ä»»å‹™é¡å‹ï¼šå› æœèªè¨€æ¨¡å‹
    # é‡å° Gemma 3 çš„æ³¨æ„åŠ›å±¤é€²è¡Œå¾®èª¿
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], 
)

# 6. å®šç¾©è¨“ç·´åƒæ•¸
training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=1,                     # è¨“ç·´çš„ Epoches æ•¸é‡
    per_device_train_batch_size=4,          # æ¯å€‹è£ç½®çš„æ‰¹æ¬¡å¤§å°
    gradient_accumulation_steps=4,          # æ¢¯åº¦ç´¯ç©æ­¥æ•¸ (ç­‰æ•ˆæ‰¹æ¬¡å¤§å°ç‚º 16)
    optim="paged_adamw_8bit",               # å„ªåŒ–å™¨ (paged_adamw_8bit ç¯€çœé¡¯å­˜)
    logging_steps=25,
    learning_rate=2e-4,                     # å­¸ç¿’ç‡
    fp16=False,
    bf16=True,                              # ä½¿ç”¨ bfloat16
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    lr_scheduler_type="constant",
    report_to="none",                       # å¯é¸ï¼šè¨­å®šç‚º "wandb" ç­‰å ±å‘Šå·¥å…·
)

# 7. åˆå§‹åŒ– SFTTrainer ä¸¦é–‹å§‹è¨“ç·´
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=lora_config,
    dataset_text_field="text",
    max_seq_length=512, # é™åˆ¶æœ€å¤§åºåˆ—é•·åº¦ï¼Œé˜²æ­¢ OOM
    tokenizer=tokenizer,
    args=training_args,
)

print("\nğŸš€ é–‹å§‹ Gemma 3 Q-LoRA å¾®èª¿...")
trainer.train()

# 8. å„²å­˜å¾®èª¿å¾Œçš„ LoRA Adapters
trainer.model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"\nâœ… å¾®èª¿å®Œæˆï¼LoRA æ¬Šé‡å·²å„²å­˜è‡³ï¼š{output_dir}")
```

#### ä½¿ç”¨å¾®èª¿å¾Œçš„ Gemma 3 æ¨¡å‹ï¼ˆå« LoRA æ¬Šé‡ï¼‰é€²è¡Œæ¨è«–å’Œåˆ†ææ–° IMDB è©•è«–çš„ç¨‹å¼ç¢¼ã€‚
- æ¨è«–çš„é—œéµåœ¨æ–¼ä½¿ç”¨ PEFT çš„ PeftModel å‡½å¼åº«ä¾†è¼‰å…¥è¨“ç·´å¥½çš„ LoRA æ¬Šé‡ï¼Œä¸¦å°‡å…¶é€£æ¥åˆ°åŸå§‹çš„ Gemma 3 æ¨¡å‹ä¸Šã€‚
- ğŸš€ æ¨è«–ç¨‹å¼ç¢¼ç¤ºä¾‹
- æ­¤ç¨‹å¼ç¢¼å°‡ä½¿ç”¨æ‚¨åœ¨ä¸Šä¸€éšæ®µå„²å­˜çš„ LoRA æ¬Šé‡ç›®éŒ„ (gemma3_imdb_sentiment_lora)
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

# --- åƒæ•¸è¨­å®š (èˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´) ---
BASE_MODEL_ID = "google/gemma-3-4b-it" 
LORA_WEIGHTS_PATH = "gemma3_imdb_sentiment_lora" # è¨“ç·´éšæ®µå„²å­˜ LoRA æ¬Šé‡çš„è·¯å¾‘

# 1. è¼‰å…¥æ¨¡å‹å’Œ LoRA æ¬Šé‡
def load_fine_tuned_model():
    print("æ­£åœ¨è¼‰å…¥åŸå§‹ Gemma 3 æ¨¡å‹ (4-bit é‡åŒ–)...")
    
    # è¨­ç½® 4-bit é‡åŒ–é…ç½® (èˆ‡è¨“ç·´æ™‚ç›¸åŒ)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    # è¼‰å…¥ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token 
    
    # è¼‰å…¥åŸå§‹ Gemma 3 æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.bfloat16,
    )

    # è¼‰å…¥ä¸¦é™„åŠ  LoRA æ¬Šé‡
    print(f"æ­£åœ¨è¼‰å…¥å¾®èª¿æ¬Šé‡ (LoRA Adapters) å¾: {LORA_WEIGHTS_PATH}")
    model = PeftModel.from_pretrained(base_model, LORA_WEIGHTS_PATH)
    
    # å¯é¸ï¼šå°‡ LoRA æ¬Šé‡èˆ‡åŸºåº•æ¨¡å‹åˆä½µï¼Œä»¥ä¾¿æ–¼éƒ¨ç½²ï¼ˆæœƒå¢åŠ é¡¯å­˜/ç¡¬ç¢Ÿä½¿ç”¨ï¼‰
    # print("æ­£åœ¨åˆä½µ LoRA æ¬Šé‡åˆ°åŸºåº•æ¨¡å‹...")
    # model = model.merge_and_unload()
    
    print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")
    return model, tokenizer

# 2. å®šç¾©æ¨è«–åˆ†æå‡½æ•¸
def analyze_review_with_gemma3(model, tokenizer, review_text):
    # **é‡è¦ï¼šä½¿ç”¨èˆ‡è¨“ç·´æ™‚å®Œå…¨ç›¸åŒçš„ Prompt æ ¼å¼**
    prompt = f"åˆ†æä»¥ä¸‹é›»å½±è©•è«–çš„æƒ…æ„Ÿå‚¾å‘ã€‚è«‹åªå›ç­” 'æ­£é¢ (Positive)' æˆ– 'è² é¢ (Negative)'ã€‚\n\n"
    prompt += f"è©•è«–: {review_text}\n\n"
    prompt += f"æƒ…æ„Ÿå‚¾å‘:" # æ¨¡å‹æ‡‰è©²å¾é€™è£¡é–‹å§‹ç”Ÿæˆ
    
    # ä½¿ç”¨èŠå¤©æ¨¡æ¿è™•ç†è¼¸å…¥
    messages = [
        {"role": "user", "content": prompt}
    ]
    input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to(model.device)
    
    # åŸ·è¡Œç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            input_ids, 
            max_new_tokens=20,          # ç¨å¾®å¤šçµ¦ä¸€äº› Token ä»¥ç¢ºä¿æ¨¡å‹å®Œæˆå›ç­”
            do_sample=False,            # ç¢ºä¿çµæœç¢ºå®šæ€§
            temperature=0.0,            # æº«åº¦è¨­ç‚º 0 ä»¥ç²å¾—æœ€å¯é çš„åˆ†é¡
        )
    
    # è§£ç¢¼æ¨¡å‹çš„å›æ‡‰
    # æˆ‘å€‘åªå–ç”Ÿæˆçš„éƒ¨åˆ†
    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)
    return response.strip().split('\n')[0] # åªå–ç¬¬ä¸€è¡Œï¼Œç¢ºä¿è¼¸å‡ºä¹¾æ·¨

# 3. åŸ·è¡Œæ¨è«–
model, tokenizer = load_fine_tuned_model()

# æ¸¬è©¦ç”¨æ–°çš„ IMDB è©•è«–
new_reviews = [
    "This film is an absolute masterpiece! The acting, the script, and the cinematography were all flawless. A must-see.",
    "I was incredibly disappointed. The plot was confusing, the pacing was agonizingly slow, and the main actor seemed completely bored.",
    "It's a decent movie for a rainy afternoon. Nothing groundbreaking, but it kept me mildly entertained. I wouldn't rush to see it again.",
]

print("-" * 50)
print("ğŸ¬ é–‹å§‹ä½¿ç”¨å¾®èª¿å¾Œçš„ Gemma 3 åˆ†ææ–°è©•è«–:")
print("-" * 50)

for i, review in enumerate(new_reviews):
    print(f"--- è©•è«– #{i+1} ---")
    print(f"åŸæ–‡: {review[:80]}...")
    
    # é€²è¡Œæƒ…æ„Ÿåˆ†æ
    prediction = analyze_review_with_gemma3(model, tokenizer, review)
    
    print(f"**Gemma 3 æƒ…æ„Ÿåˆ†æçµæœ:** {prediction}")
    print("-" * 20)
```
- é—œéµæ­¥é©Ÿè§£æ
  - è¼‰å…¥åŸºåº•æ¨¡å‹èˆ‡ LoRA (PeftModel.from_pretrained)
  - é¦–å…ˆï¼Œæˆ‘å€‘ä½¿ç”¨èˆ‡è¨“ç·´æ™‚ç›¸åŒçš„ BitsAndBytesConfig è¼‰å…¥åŸå§‹çš„ Gemma 3 åŸºåº•æ¨¡å‹ã€‚
  - æ¥è‘—ï¼Œä½¿ç”¨ PeftModel.from_pretrained(base_model, LORA_WEIGHTS_PATH) å°‡æ‚¨å¾®èª¿çš„ LoRA æ¬Šé‡æª”æ¡ˆï¼ˆä½æ–¼ gemma3_imdb_sentiment_lora ç›®éŒ„å…§ï¼‰æ›è¼‰åˆ°åŸºåº•æ¨¡å‹ä¸Šã€‚é€™æ¨£ï¼Œæ¨¡å‹åœ¨æ¨è«–æ™‚å°±æœƒåŒæ™‚åˆ©ç”¨åŸå§‹çŸ¥è­˜å’Œå¾®èª¿å­¸åˆ°çš„æ–°çŸ¥è­˜ã€‚
  - æ¨è«– Prompt æ ¼å¼
  - ç¢ºä¿åœ¨ analyze_review_with_gemma3 å‡½æ•¸ä¸­ä½¿ç”¨çš„ Prompt (æç¤ºè©) æ ¼å¼èˆ‡æ‚¨åœ¨è¨“ç·´éšæ®µ (format_imdb_data å‡½æ•¸) ä¸­ä½¿ç”¨çš„æ ¼å¼å®Œå…¨ä¸€æ¨£ã€‚æ ¼å¼ä¸ä¸€è‡´æ˜¯æ¨è«–å¤±æ•—æˆ–æº–ç¢ºç‡ä½ä¸‹çš„æœ€å¸¸è¦‹åŸå› ã€‚
  - å°‡ temperature è¨­ç½®ç‚º 0.0 ç¢ºä¿æ¨¡å‹è¼¸å‡ºæœ€å¯é çš„åˆ†é¡çµæœï¼Œè€Œä¸æ˜¯ç™¼æ®å‰µé€ åŠ›ã€‚
  - é€éé€™å€‹æ–¹æ³•ï¼Œæ‚¨çš„ Gemma 3 æ¨¡å‹ç¾åœ¨å·²ç¶“å°ˆé–€é‡å° IMDB è©•è«–æƒ…æ„Ÿåˆ†æä»»å‹™é€²è¡Œäº†å„ªåŒ–ï¼Œæ‡‰è©²èƒ½å¤ æä¾›æ¯”åŸå§‹æ¨¡å‹æ›´æº–ç¢ºçš„çµæœã€‚
