# 摘要生成範例程式
- 示範如何使用 Gemma 3 (Instruction-Tuned) 模型來生成摘要（Summarization）。
- 由於生成摘要是一個複雜的文本生成任務，我們將再次使用 Gemma 3 的指令微調版本 (google/gemma-3-4b-it)，因為它在遵循指令和生成連貫文本方面表現優異。

## 💻 摘要生成程式碼示例
- 此程式碼將載入 Gemma 3 模型，並利用一個精心設計的 Prompt 來要求模型為一段長文本生成一份簡潔的摘要。

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# --- 參數設定 ---
MODEL_ID = "google/gemma-3-4b-it" 

# 1. 設置 4-bit 量化配置 (與先前保持一致，以節省顯存)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# 2. 載入模型和 Tokenizer
print("正在載入 Gemma 3 模型 (指令微調版) 進行摘要任務...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token 

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)
print("✅ 模型載入完成。")

# 3. 定義摘要生成函數
def generate_summary(text_to_summarize, max_length=150):
    
    # 設計摘要 Prompt：明確要求模型的輸出格式和限制
    prompt = f"請為以下這段文字生成一份簡潔、客觀的摘要。請將摘要限制在四個句子以內，且不添加額外的評論。\n\n"
    prompt += f"原文:\n{text_to_summarize}\n\n"
    prompt += f"摘要:"
    
    # 設置模型的輸入 (使用聊天模板)
    messages = [
        {"role": "user", "content": prompt}
    ]
    input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to(model.device)
    
    # 執行生成
    with torch.no_grad():
        outputs = model.generate(
            input_ids, 
            max_new_tokens=max_length,         # 限制摘要的長度
            do_sample=True,                    # 啟用採樣以生成更自然流暢的文本
            temperature=0.7,                   # 設置溫度以增加文本的多樣性和自然度
            top_p=0.9,
            repetition_penalty=1.1,            # 避免重複
        )
    
    # 解碼模型的回應
    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)
    return response.strip()

# 4. 測試範例：使用一段較長的電影評論或報導
sample_text = """
李安執導的《臥虎藏龍》在 2000 年上映後，迅速成為全球文化現象。這部電影不僅在武術設計上達到了前所未有的美學高度，將中國武俠片的內涵提升到了哲學層面，更成功地將東方美學推向了西方主流觀眾。電影的核心故事圍繞著玉嬌龍和李慕白之間的命運糾葛展開，探討了自由、責任與情感壓抑等普世主題。

它在美國取得了驚人的票房成績，最終成為歷史上最成功的非英語電影之一。更重要的是，它在第 73 屆奧斯卡金像獎上榮獲最佳外語片等多項大獎，證明了其跨越文化界限的魅力。然而，電影中緩慢的敘事節奏和對傳統武俠片形式的顛覆，也引起了部分傳統觀眾的爭議。儘管如此，它的影響力至今仍在，啟發了後續許多跨文化、跨類型的電影創作。
"""

print("-" * 60)
print("📚 待摘要原文:")
print(sample_text)
print("-" * 60)

# 生成摘要
summary = generate_summary(sample_text)

print("📝 Gemma 3 生成的摘要 (溫度 0.7):")
print(summary)
print("-" * 60)
```

## 好的，很樂意為您示範如何使用 Gemma 3 (Instruction-Tuned) 模型來生成摘要（Summarization）。

由於生成摘要是一個複雜的文本生成任務，我們將再次使用 Gemma 3 的指令微調版本 (google/gemma-3-4b-it)，因為它在遵循指令和生成連貫文本方面表現優異。

💻 摘要生成程式碼示例
此程式碼將載入 Gemma 3 模型，並利用一個精心設計的 Prompt 來要求模型為一段長文本生成一份簡潔的摘要。

Python

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# --- 參數設定 ---
MODEL_ID = "google/gemma-3-4b-it" 

# 1. 設置 4-bit 量化配置 (與先前保持一致，以節省顯存)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# 2. 載入模型和 Tokenizer
print("正在載入 Gemma 3 模型 (指令微調版) 進行摘要任務...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token 

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)
print("✅ 模型載入完成。")

# 3. 定義摘要生成函數
def generate_summary(text_to_summarize, max_length=150):
    
    # 設計摘要 Prompt：明確要求模型的輸出格式和限制
    prompt = f"請為以下這段文字生成一份簡潔、客觀的摘要。請將摘要限制在四個句子以內，且不添加額外的評論。\n\n"
    prompt += f"原文:\n{text_to_summarize}\n\n"
    prompt += f"摘要:"
    
    # 設置模型的輸入 (使用聊天模板)
    messages = [
        {"role": "user", "content": prompt}
    ]
    input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to(model.device)
    
    # 執行生成
    with torch.no_grad():
        outputs = model.generate(
            input_ids, 
            max_new_tokens=max_length,         # 限制摘要的長度
            do_sample=True,                    # 啟用採樣以生成更自然流暢的文本
            temperature=0.7,                   # 設置溫度以增加文本的多樣性和自然度
            top_p=0.9,
            repetition_penalty=1.1,            # 避免重複
        )
    
    # 解碼模型的回應
    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)
    return response.strip()

# 4. 測試範例：使用一段較長的電影評論或報導
sample_text = """
李安執導的《臥虎藏龍》在 2000 年上映後，迅速成為全球文化現象。這部電影不僅在武術設計上達到了前所未有的美學高度，將中國武俠片的內涵提升到了哲學層面，更成功地將東方美學推向了西方主流觀眾。電影的核心故事圍繞著玉嬌龍和李慕白之間的命運糾葛展開，探討了自由、責任與情感壓抑等普世主題。

它在美國取得了驚人的票房成績，最終成為歷史上最成功的非英語電影之一。更重要的是，它在第 73 屆奧斯卡金像獎上榮獲最佳外語片等多項大獎，證明了其跨越文化界限的魅力。然而，電影中緩慢的敘事節奏和對傳統武俠片形式的顛覆，也引起了部分傳統觀眾的爭議。儘管如此，它的影響力至今仍在，啟發了後續許多跨文化、跨類型的電影創作。
"""

print("-" * 60)
print("📚 待摘要原文:")
print(sample_text)
print("-" * 60)

# 生成摘要
summary = generate_summary(sample_text)

print("📝 Gemma 3 生成的摘要 (溫度 0.7):")
print(summary)
print("-" * 60)
```

## 關鍵概念說明
- Prompt Engineering for Summarization (摘要提示詞設計)

在 Prompt 中，我們明確指示模型完成的任務是「生成一份簡潔、客觀的摘要」。

我們加入了約束條件：「將摘要限制在四個句子以內，且不添加額外的評論」，這能讓模型輸出更符合您的預期格式。

生成參數 (Generation Parameters)

do_sample=True 和 temperature=0.7: 這是生成摘要或創意文本的常用設置。與先前分類任務中使用的 temperature=0.0 不同，這裡的較高溫度允許模型在多個可能的詞語中進行選擇，使生成的摘要更具人類寫作的流暢性和自然感。

max_new_tokens: 限制了模型輸出的最大長度，確保摘要不會過長。

透過調整 Prompt 和這些生成參數，您可以控制 Gemma 3 生成摘要的風格、長度和客觀性，以滿足不同的應用需求。
