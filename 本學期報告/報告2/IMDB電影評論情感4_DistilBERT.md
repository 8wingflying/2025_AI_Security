#

```python
# ğŸ¬ TensorFlow Ã— DistilBERT IMDB æƒ…æ„Ÿåˆ†ææ•™å­¸
> ä½¿ç”¨ TensorFlow + Hugging Face Transformers å¾®èª¿ DistilBERT æ¨¡å‹æ–¼ IMDB é›»å½±è©•è«–æƒ…æ„Ÿåˆ†æ  
> ä½œè€…ï¼šT Ben Ã— ChatGPT GPT-5 ï½œ æ—¥æœŸï¼š2025-10-30  

---

## ğŸ§  ä¸€ã€ç†è«–ç°¡ä»‹

### ğŸ” ä»€éº¼æ˜¯ DistilBERTï¼Ÿ
**DistilBERT** æ˜¯ç”± **BERT** ç²¾ç°¡è€Œä¾†çš„è¼•é‡ Transformer æ¨¡å‹ã€‚  
é€é **çŸ¥è­˜è’¸é¤¾ï¼ˆKnowledge Distillationï¼‰** æŠ€è¡“ï¼Œ  
å°æ¨¡å‹å¯å­¸ç¿’å¤§æ¨¡å‹çš„èªæ„è¡¨å¾µï¼Œä¿ç•™ç´„ 97% çš„æ€§èƒ½ä½†é€Ÿåº¦å¿«ç´„ 60%ã€‚

### ğŸŒŸ å„ªé»
- âš¡ è¨“ç·´èˆ‡æ¨è«–é€Ÿåº¦æ›´å¿«  
- ğŸ§© æ¨¡å‹é«”ç©æ›´å°ï¼Œé©åˆéƒ¨ç½²  
- ğŸ¯ æ•ˆèƒ½æ¥è¿‘åŸå§‹ BERT  

### ğŸ’¡ æ‡‰ç”¨å ´æ™¯
- å½±è©•æˆ–å•†å“è©•è«–çš„æƒ…æ„Ÿåˆ†æ  
- æ–‡å­—åˆ†é¡èˆ‡æ„åœ–è­˜åˆ¥  
- FAQï¼å®¢æœèŠå¤©æ©Ÿå™¨äºº  

---

## âš™ï¸ äºŒã€ç’°å¢ƒå®‰è£

```bash
pip install tensorflow transformers datasets
```
# ğŸ“¦ ä¸‰ã€è³‡æ–™è¼‰å…¥èˆ‡å‰è™•ç†

```PYTHON
import tensorflow as tf
from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification
from datasets import load_dataset

# 1ï¸âƒ£ è¼‰å…¥ IMDB è³‡æ–™é›†
imdb = load_dataset("imdb")

# 2ï¸âƒ£ å–å‡ºè¨“ç·´èˆ‡æ¸¬è©¦è³‡æ–™
train_texts = imdb["train"]["text"]
train_labels = imdb["train"]["label"]
test_texts  = imdb["test"]["text"]
test_labels = imdb["test"]["label"]

# 3ï¸âƒ£ è¼‰å…¥ DistilBERT Tokenizer
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

# 4ï¸âƒ£ Tokenize è³‡æ–™
def preprocess(texts, labels, max_length=128):
    enc = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="tf"
    )
    return {"input_ids": enc["input_ids"], "attention_mask": enc["attention_mask"]}, tf.convert_to_tensor(labels)

train_inputs, train_labels_tf = preprocess(train_texts, train_labels)
test_inputs,  test_labels_tf  = preprocess(test_texts,  test_labels)

# ğŸ§  å››ã€å»ºç«‹èˆ‡å¾®èª¿ DistilBERT æ¨¡å‹

# 1ï¸âƒ£ è¼‰å…¥é è¨“ç·´ DistilBERT æ¨¡å‹
model = TFDistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2
)

# 2ï¸âƒ£ ç·¨è­¯æ¨¡å‹
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)
model.compile(
    optimizer=optimizer,
    loss=model.compute_loss,  # Transformers æ¨¡å‹è‡ªå¸¶ loss
    metrics=["accuracy"]
)

# 3ï¸âƒ£ è¨“ç·´æ¨¡å‹
history = model.fit(
    x=train_inputs,
    y=train_labels_tf,
    validation_split=0.1,
    epochs=3,
    batch_size=16
)

# 4ï¸âƒ£ è©•ä¼°æ¨¡å‹
loss, acc = model.evaluate(test_inputs, test_labels_tf)
print(f"æ¸¬è©¦æº–ç¢ºç‡ï¼š{acc:.4f}")

#ğŸ¯ äº”ã€é æ¸¬æ–°è©•è«–

def predict_review(text):
    enc = tokenizer(text, padding=True, truncation=True, return_tensors="tf", max_length=128)
    logits = model(enc["input_ids"], enc["attention_mask"])[0]
    pred = tf.math.argmax(logits, axis=1).numpy()[0]
    return "æ­£é¢è©•è«– ğŸ‘" if pred == 1 else "è² é¢è©•è«– ğŸ‘"

print(predict_review("This movie was fantastic! I really enjoyed it."))
print(predict_review("I hated this film. It was boring and too long."))
```
## ğŸ“Š å…­ã€æ¨¡å‹æ¯”è¼ƒè¡¨

```
æ¨¡å‹	æ¶æ§‹	å„ªé»	ç¼ºé»	æº–ç¢ºç‡ (ç´„)
GRU	RNN	è¼•é‡å¿«é€Ÿ	é›£æ•æ‰é•·èªå¢ƒ	0.85
BiLSTM	é›™å‘ LSTM	èªæ„ç†è§£å¼·	è¨“ç·´æ…¢	0.88
Transformer	Self-Attention	ä¸¦è¡Œé«˜æ•ˆ	è³‡æ–™éœ€æ±‚å¤§	0.91
DistilBERT	ç²¾ç°¡ BERT	å¿«é€Ÿç²¾æº–å…¼é¡§	éœ€ GPU è¨“ç·´	0.93â€“0.95
```
## ğŸ§­ ä¸ƒã€å»¶ä¼¸ç·´ç¿’èˆ‡å„ªåŒ–å»ºè­°
```
è¶…åƒæ•¸å¾®èª¿

å˜—è©¦ä¸åŒçš„ max_lengthã€batch_sizeã€learning_rateã€‚

å‡çµéƒ¨åˆ†å±¤

åªè¨“ç·´æœ€å¾Œå¹¾å±¤ä»¥åŠ å¿« fine-tuningã€‚

å¤šèªè¨€æ”¯æ´

ä½¿ç”¨ distilbert-base-multilingual-cased ä»¥æ”¯æ´ä¸­è‹±æ··åˆè©•è«–ã€‚

æ³¨æ„åŠ›å¯è¦–åŒ–

é¡¯ç¤º Attention Mapï¼Œè§€å¯Ÿæ¨¡å‹é—œæ³¨å“ªäº›è©ã€‚

è‡ªè¨‚è³‡æ–™é›†

å°‡ä¸­æ–‡å½±è©•ã€å•†å“è©•è«–å°å…¥å†è¨“ç·´ï¼Œå»ºç«‹åœ¨åœ°åŒ–æƒ…æ„Ÿæ¨¡å‹ã€‚
```

# ğŸ å…«ã€çµè«–
```
DistilBERT æ˜¯å…¼å…·é€Ÿåº¦èˆ‡æº–ç¢ºç‡çš„é«˜æ•ˆ NLP æ¨¡å‹ã€‚
åœ¨ IMDB ä»»å‹™ä¸­ï¼Œå®ƒèƒ½ä»¥ç´„ä¸€åŠåƒæ•¸é”æˆæ¥è¿‘ BERT çš„æ•ˆæœï¼ˆç´„ 93%â€“95% æº–ç¢ºç‡ï¼‰ã€‚
éå¸¸é©åˆæ‡‰ç”¨æ–¼ï¼š

å®¢æœæ„åœ–è­˜åˆ¥

å³æ™‚è©•è«–åˆ†æ

è¡Œå‹•è£ç½®ç«¯ NLP ä»»å‹™
```
