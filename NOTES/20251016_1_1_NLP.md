## NLP

## 參考
- https://github.com/PrestonYU/Natural-Language-Processing-with-Python
- https://github.com/IKMLab/NTHU_Natural_Language_Processing
- https://web.stanford.edu/~jurafsky/slp3/ed3book_aug25.pdf

## 自然語言處理
- 自然語言處理通常包括以下步驟：
  - 文字預處理：清理和準備文字數據，例如移除特殊字元、轉換為小寫等。
  - 分詞（Tokenization）：將文字分成單字、片語或句子。
  - 詞性標註（Part-of-Speech Tagging）：為文本中的單字標註詞性，如名詞、動詞、形容詞等。
  - 命名實體識別（Named Entity Recognition）：識別文字中的命名實體，如人名、地名、日期等。
  - 句法分析（Syntax Parsing）：分析文本中的句子結構，包括主謂賓關係、子句等。
  - 情緒分析（Sentiment Analysis）：辨識文本中的情緒傾向，如正向、負向或中性。

## NLTK
- 🌸 1. 斷詞與停用詞過濾
```python
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

text = "Natural Language Processing with Python and NLTK is very powerful."
tokens = word_tokenize(text)

# 移除停用詞
stop_words = set(stopwords.words('english'))
filtered = [w for w in tokens if w.lower() not in stop_words]

print("Tokens:", tokens)
print("Filtered:", filtered)

```
- 🌱 2. 詞幹提取與詞形還原
- 🔸 詞幹提取（stemming）只用規則；詞形還原（lemmatization）則依據詞典與語意。
```python
from nltk.stem import PorterStemmer, WordNetLemmatizer

ps = PorterStemmer()
lemmatizer = WordNetLemmatizer()

words = ["running", "flies", "better", "studies"]

for w in words:
    print(f"{w:10} → Stem: {ps.stem(w):10} | Lemma: {lemmatizer.lemmatize(w)}")

```
- 🧩 3. 詞性標註（POS Tagging）

```python
from nltk import pos_tag, word_tokenize

text = "John loves playing football with his friends."
tokens = word_tokenize(text)
tags = pos_tag(tokens)

print(tags)
```

- 🧠 4. 命名實體辨識（NER）
```python
from nltk import ne_chunk, pos_tag, word_tokenize
from nltk.tree import Tree

text = "Barack Obama was born in Hawaii and worked at the White House."
tokens = word_tokenize(text)
tags = pos_tag(tokens)
tree = ne_chunk(tags)

for subtree in tree:
    if type(subtree) == Tree:
        print(subtree.label(), ' '.join(c[0] for c in subtree))
```

```PYTHON
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk

# 文本處理
text = "The quick brown fox jumps over the lazy dog."

# 斷詞
tokens = word_tokenize(text)

# 詞性標注
tagged = pos_tag(tokens)

# 命名實體識別
named_entities = ne_chunk(tagged)

# 輸出结果
print("Tokens:", tokens)
print("Tagged:", tagged)
print("Named Entities:", named_entities)
```
## Tokenization:BPE| Byte Pair Encoding
- https://www.youtube.com/watch?v=NrT5kmnTFCk
## 文字 -- > 向量(Vector)
- 傳統作法: one-hot encoding
- 現代做法:
  - Word2Vec	Google (Mikolov, 2013)	透過神經網路預測上下文或中心詞	訓練速度快、效果好
  - GloVe (Global Vectors)	Stanford (Pennington, 2014)	利用詞共現矩陣（co-occurrence matrix）統計全域語意	保留全局語義關係
  - FastText	Facebook (2016)	把詞拆成字詞（subword）訓練	支援未知詞 (OOV) 
- 上下文感知（contextualized）的詞嵌入模型|Contextualized Word Embedding
  - ELMo(2018)	Allen Institute (AI2)	動態（Contextualized）	透過上下文生成不同嵌入	Bi-directional LSTM
  - BERT(2018)	Google	動態（Contextualized）	使用 Transformer 雙向編碼上下文	Transformer Encoder
  - CLIP	2021	OpenAI	多模態（Text + Image）	對齊文字與影像的語意空間	Contrastive Learning (ViT + Text Transformer)

#### BERT 生成詞嵌入
```python
from transformers import BertTokenizer, BertModel
import torch

# 載入模型與分詞器
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# 輸入句子
text = "He went to the bank to deposit money."
inputs = tokenizer(text, return_tensors='pt')
outputs = model(**inputs)

# 取得最後一層嵌入
embeddings = outputs.last_hidden_state
print(embeddings.shape)  # [1, token數, 768]
```
