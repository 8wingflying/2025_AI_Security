## NLP

## åƒè€ƒ
- https://github.com/PrestonYU/Natural-Language-Processing-with-Python
- https://github.com/IKMLab/NTHU_Natural_Language_Processing
- https://web.stanford.edu/~jurafsky/slp3/ed3book_aug25.pdf

## è‡ªç„¶èªè¨€è™•ç†
- è‡ªç„¶èªè¨€è™•ç†é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹æ­¥é©Ÿï¼š
  - æ–‡å­—é è™•ç†ï¼šæ¸…ç†å’Œæº–å‚™æ–‡å­—æ•¸æ“šï¼Œä¾‹å¦‚ç§»é™¤ç‰¹æ®Šå­—å…ƒã€è½‰æ›ç‚ºå°å¯«ç­‰ã€‚
  - åˆ†è©ï¼ˆTokenizationï¼‰ï¼šå°‡æ–‡å­—åˆ†æˆå–®å­—ã€ç‰‡èªæˆ–å¥å­ã€‚
  - è©æ€§æ¨™è¨»ï¼ˆPart-of-Speech Taggingï¼‰ï¼šç‚ºæ–‡æœ¬ä¸­çš„å–®å­—æ¨™è¨»è©æ€§ï¼Œå¦‚åè©ã€å‹•è©ã€å½¢å®¹è©ç­‰ã€‚
  - å‘½åå¯¦é«”è­˜åˆ¥ï¼ˆNamed Entity Recognitionï¼‰ï¼šè­˜åˆ¥æ–‡å­—ä¸­çš„å‘½åå¯¦é«”ï¼Œå¦‚äººåã€åœ°åã€æ—¥æœŸç­‰ã€‚
  - å¥æ³•åˆ†æï¼ˆSyntax Parsingï¼‰ï¼šåˆ†ææ–‡æœ¬ä¸­çš„å¥å­çµæ§‹ï¼ŒåŒ…æ‹¬ä¸»è¬‚è³“é—œä¿‚ã€å­å¥ç­‰ã€‚
  - æƒ…ç·’åˆ†æï¼ˆSentiment Analysisï¼‰ï¼šè¾¨è­˜æ–‡æœ¬ä¸­çš„æƒ…ç·’å‚¾å‘ï¼Œå¦‚æ­£å‘ã€è² å‘æˆ–ä¸­æ€§ã€‚

## NLTK
- ğŸŒ¸ 1. æ–·è©èˆ‡åœç”¨è©éæ¿¾
```python
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

text = "Natural Language Processing with Python and NLTK is very powerful."
tokens = word_tokenize(text)

# ç§»é™¤åœç”¨è©
stop_words = set(stopwords.words('english'))
filtered = [w for w in tokens if w.lower() not in stop_words]

print("Tokens:", tokens)
print("Filtered:", filtered)

```
- ğŸŒ± 2. è©å¹¹æå–èˆ‡è©å½¢é‚„åŸ
- ğŸ”¸ è©å¹¹æå–ï¼ˆstemmingï¼‰åªç”¨è¦å‰‡ï¼›è©å½¢é‚„åŸï¼ˆlemmatizationï¼‰å‰‡ä¾æ“šè©å…¸èˆ‡èªæ„ã€‚
```python
from nltk.stem import PorterStemmer, WordNetLemmatizer

ps = PorterStemmer()
lemmatizer = WordNetLemmatizer()

words = ["running", "flies", "better", "studies"]

for w in words:
    print(f"{w:10} â†’ Stem: {ps.stem(w):10} | Lemma: {lemmatizer.lemmatize(w)}")

```
- ğŸ§© 3. è©æ€§æ¨™è¨»ï¼ˆPOS Taggingï¼‰

```python
from nltk import pos_tag, word_tokenize

text = "John loves playing football with his friends."
tokens = word_tokenize(text)
tags = pos_tag(tokens)

print(tags)
```

- ğŸ§  4. å‘½åå¯¦é«”è¾¨è­˜ï¼ˆNERï¼‰
```python
from nltk import ne_chunk, pos_tag, word_tokenize
from nltk.tree import Tree

text = "Barack Obama was born in Hawaii and worked at the White House."
tokens = word_tokenize(text)
tags = pos_tag(tokens)
tree = ne_chunk(tags)

for subtree in tree:
    if type(subtree) == Tree:
        print(subtree.label(), ' '.join(c[0] for c in subtree))
```

```PYTHON
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk

# æ–‡æœ¬è™•ç†
text = "The quick brown fox jumps over the lazy dog."

# æ–·è©
tokens = word_tokenize(text)

# è©æ€§æ¨™æ³¨
tagged = pos_tag(tokens)

# å‘½åå¯¦é«”è­˜åˆ¥
named_entities = ne_chunk(tagged)

# è¼¸å‡ºç»“æœ
print("Tokens:", tokens)
print("Tagged:", tagged)
print("Named Entities:", named_entities)
```
## Tokenization:BPE| Byte Pair Encoding
- https://www.youtube.com/watch?v=NrT5kmnTFCk
## æ–‡å­— -- > å‘é‡(Vector)
- å‚³çµ±ä½œæ³•: one-hot encoding
- ç¾ä»£åšæ³•:
  - Word2Vec	Google (Mikolov, 2013)	é€éç¥ç¶“ç¶²è·¯é æ¸¬ä¸Šä¸‹æ–‡æˆ–ä¸­å¿ƒè©	è¨“ç·´é€Ÿåº¦å¿«ã€æ•ˆæœå¥½
  - GloVe (Global Vectors)	Stanford (Pennington, 2014)	åˆ©ç”¨è©å…±ç¾çŸ©é™£ï¼ˆco-occurrence matrixï¼‰çµ±è¨ˆå…¨åŸŸèªæ„	ä¿ç•™å…¨å±€èªç¾©é—œä¿‚
  - FastText	Facebook (2016)	æŠŠè©æ‹†æˆå­—è©ï¼ˆsubwordï¼‰è¨“ç·´	æ”¯æ´æœªçŸ¥è© (OOV) 
- ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆcontextualizedï¼‰çš„è©åµŒå…¥æ¨¡å‹|Contextualized Word Embedding
  - ELMo(2018)	Allen Institute (AI2)	å‹•æ…‹ï¼ˆContextualizedï¼‰	é€éä¸Šä¸‹æ–‡ç”Ÿæˆä¸åŒåµŒå…¥	Bi-directional LSTM
  - BERT(2018)	Google	å‹•æ…‹ï¼ˆContextualizedï¼‰	ä½¿ç”¨ Transformer é›™å‘ç·¨ç¢¼ä¸Šä¸‹æ–‡	Transformer Encoder
  - CLIP	2021	OpenAI	å¤šæ¨¡æ…‹ï¼ˆText + Imageï¼‰	å°é½Šæ–‡å­—èˆ‡å½±åƒçš„èªæ„ç©ºé–“	Contrastive Learning (ViT + Text Transformer)

#### BERT ç”Ÿæˆè©åµŒå…¥
```python
from transformers import BertTokenizer, BertModel
import torch

# è¼‰å…¥æ¨¡å‹èˆ‡åˆ†è©å™¨
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# è¼¸å…¥å¥å­
text = "He went to the bank to deposit money."
inputs = tokenizer(text, return_tensors='pt')
outputs = model(**inputs)

# å–å¾—æœ€å¾Œä¸€å±¤åµŒå…¥
embeddings = outputs.last_hidden_state
print(embeddings.shape)  # [1, tokenæ•¸, 768]
```
