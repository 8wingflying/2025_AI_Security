## CNN 常用元件
- Module: tf.keras.layers | TensorFlow v2.16.1
  - https://www.tensorflow.org/api_docs/python/tf/keras/layers 
- tf.keras.layers.Conv2D
- tf.keras.layers.MaxPool2D
- tf.keras.layers.Dropout

# 檢視CNN 模型
- Module: tf.keras.applications
  - https://www.tensorflow.org/api_docs/python/tf/keras/applications 
- 檢視VGG16模型
- https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16
```python
from tf.keras.applications import VGG16
model = VGG16()
model.summary()
```

![VGG16.png](VGG16.png)

## 簡易資料集
- Kaggle | [MNIST Dataset The MNIST database of handwritten digits (http://yann.lecun.com)](https://www.kaggle.com/datasets/hojjatk/mnist-dataset)
- Kaggle | [Fashion MNIST:An MNIST-like dataset of 70,000 28x28 labeled fashion images](https://www.kaggle.com/datasets/zalando-research/fashionmnist)


# 作業:CNN模型實作
- 請使用keras sequential model 實作lenet-5 資料集用MINST

#### ChatGPT 同學繳交的報告 ==> 理解模型錯誤 ==> AI Hallucination (幻覺)
- Prompt :請使用keras sequential model 實作lenet-5 資料集用MINST
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

# 載入 MNIST 資料集
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 調整資料形狀 (28x28 → 28x28x1)
x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
x_test  = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0

# One-hot 編碼標籤
y_train = to_categorical(y_train, 10)
y_test  = to_categorical(y_test, 10)

# 建立 LeNet-5 模型
model = Sequential([
    Conv2D(6, kernel_size=(5,5), activation='tanh', input_shape=(28,28,1), padding='same'),
    AveragePooling2D(pool_size=(2,2), strides=2),

    Conv2D(16, kernel_size=(5,5), activation='tanh'),
    AveragePooling2D(pool_size=(2,2), strides=2),

## 這不是LeNet-5 模型 ==> AI Hallucination (幻覺)
    Conv2D(120, kernel_size=(5,5), activation='tanh'),
    Flatten(),
    Dense(84, activation='tanh'),
    Dense(10, activation='softmax')
])

# 編譯模型 ==> 使用compile()
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# 訓練模型  ==> 使用fit()
model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.1)

# 評估模型
test_loss, test_acc = model.evaluate(x_test, y_test)
print("Test accuracy:", test_acc)
```
#### Google Gemini同學繳交的報告
- Prompt 1:請使用keras sequential model 實作lenet-5 資料集用MINST
- Prompt 2:匯出到Google Colab
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical
import numpy as np

# 確保 TensorFlow 版本正確
print(f"TensorFlow 版本: {tf.__version__}")

# --- 1. 資料載入與預處理 (Data Loading and Preprocessing) ---

# 載入 MNIST 資料集
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# 1. 重塑 (Reshape) 資料以包含顏色通道 (channel dimension)
# Keras 需要的輸入格式為 (樣本數, 高度, 寬度, 通道數)。灰階圖片通道數為 1。
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')

# 2. 數據正規化 (Normalization)
# 將像素值從 0-255 縮放到 0-1
x_train /= 255.0
x_test /= 255.0

# 3. 標籤獨熱編碼 (One-Hot Encoding)
# 將類別標籤 (0-9) 轉換為獨熱向量 (One-Hot Vector)
num_classes = 10
y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)

# 定義模型輸入形狀
input_shape = (28, 28, 1)


# --- 2. 定義 LeNet-5 模型架構 (Define LeNet-5 Architecture) ---

def build_lenet_5(input_shape, num_classes):
    model = Sequential()
    
    # 原始 LeNet-5 使用 tanh 激活函數和 AveragePooling2D
    
    # C1: 卷積層
    model.add(Conv2D(
        filters=6, 
        kernel_size=(5, 5), 
        activation='tanh', 
        input_shape=input_shape, 
        padding='valid' 
    ))
    
    # S2: 平均池化層 (Subsampling Layer)
    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))
    
    # C3: 卷積層
    model.add(Conv2D(
        filters=16, 
        kernel_size=(5, 5), 
        activation='tanh',
        padding='valid'
    ))
    
    # S4: 平均池化層 (Subsampling Layer)
    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))
    
    # 展平層 (Flatten) - 轉換為 1D 向量
    model.add(Flatten())
    
    # F5: 全連接層 (Fully Connected Layer)
    model.add(Dense(units=120, activation='tanh'))
    
    # F6: 全連接層
    model.add(Dense(units=84, activation='tanh'))
    
    # 輸出層 (Output Layer) - 使用 softmax 進行分類
    model.add(Dense(units=num_classes, activation='softmax'))
    
    return model

# 建立模型實例
model = build_lenet_5(input_shape, num_classes)

# 打印模型摘要
print("\n--- LeNet-5 模型摘要 ---")
model.summary()

# --- 3. 編譯與訓練模型 (Compile and Train Model) ---

# 模型編譯
model.compile(
    optimizer='adam',                 # 使用 Adam 優化器 (現代常見選擇)
    loss='categorical_crossentropy',  # 適用於多類別分類
    metrics=['accuracy']
)

# 訓練參數
epochs = 10
batch_size = 128

print("\n--- 開始訓練 ---")
history = model.fit(
    x_train, 
    y_train, 
    epochs=epochs, 
    batch_size=batch_size, 
    verbose=1,
    validation_data=(x_test, y_test)
)
print("--- 訓練結束 ---")

# --- 4. 模型評估 (Model Evaluation) ---

print("\n--- 模型最終評估 ---")
score = model.evaluate(x_test, y_test, verbose=0)
print(f'測試集損失 (Test Loss): {score[0]:.4f}')
print(f'測試集準確度 (Test Accuracy): {score[1]:.4f}')
```

#### Claude Sonnet同學繳交的報告
```python
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

# 載入 MNIST 資料集
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 資料前處理
# LeNet-5 原始輸入為 32x32，這裡將 MNIST 的 28x28 填充到 32x32
x_train = np.pad(x_train, ((0,0), (2,2), (2,2)), mode='constant', constant_values=0)
x_test = np.pad(x_test, ((0,0), (2,2), (2,2)), mode='constant', constant_values=0)

# 正規化到 [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# 增加通道維度 (samples, height, width, channels)
x_train = np.expand_dims(x_train, axis=-1)
x_test = np.expand_dims(x_test, axis=-1)

# 標籤轉為 one-hot encoding
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

print(f"訓練集形狀: {x_train.shape}")
print(f"測試集形狀: {x_test.shape}")

# 建立 LeNet-5 模型
model = keras.Sequential([
    # C1: 卷積層 - 6個 5x5 filters
    layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), 
                  activation='tanh', input_shape=(32, 32, 1), padding='valid'),
    
    # S2: 平均池化層
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),
    
    # C3: 卷積層 - 16個 5x5 filters
    layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), 
                  activation='tanh', padding='valid'),
    
    # S4: 平均池化層
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),
    
    # C5: 卷積層 - 120個 5x5 filters
    layers.Conv2D(120, kernel_size=(5, 5), strides=(1, 1), 
                  activation='tanh', padding='valid'),
    
    # 展平層
    layers.Flatten(),
    
    # F6: 全連接層 - 84個神經元
    layers.Dense(84, activation='tanh'),
    
    # 輸出層 - 10個類別
    layers.Dense(10, activation='softmax')
])

# 編譯模型
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# 顯示模型架構
model.summary()

# 訓練模型
print("\n開始訓練...")
history = model.fit(
    x_train, y_train,
    batch_size=128,
    epochs=10,
    validation_split=0.1,
    verbose=1
)

# 評估模型
print("\n評估測試集...")
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(f"測試準確率: {test_acc*100:.2f}%")
print(f"測試損失: {test_loss:.4f}")

# 繪製訓練歷史
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# 準確率
ax1.plot(history.history['accuracy'], label='訓練準確率')
ax1.plot(history.history['val_accuracy'], label='驗證準確率')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Accuracy')
ax1.set_title('模型準確率')
ax1.legend()
ax1.grid(True)

# 損失
ax2.plot(history.history['loss'], label='訓練損失')
ax2.plot(history.history['val_loss'], label='驗證損失')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Loss')
ax2.set_title('模型損失')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.show()

# 預測範例
predictions = model.predict(x_test[:10])
predicted_labels = np.argmax(predictions, axis=1)
true_labels = np.argmax(y_test[:10], axis=1)

# 顯示前10個測試樣本的預測結果
fig, axes = plt.subplots(2, 5, figsize=(12, 5))
for i, ax in enumerate(axes.flat):
    ax.imshow(x_test[i].squeeze(), cmap='gray')
    ax.set_title(f'預測: {predicted_labels[i]}\n實際: {true_labels[i]}')
    ax.axis('off')
plt.tight_layout()
plt.show()
```
