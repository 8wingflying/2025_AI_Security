{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FtbRuQ7n2X8e"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"Natural Language Processing with Python and NLTK is very powerful.\"\n",
        "tokens = word_tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9efac242",
        "outputId": "63fe488b-8ffb-44b6-9d76-c1e464d1ff0b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da91159c",
        "outputId": "8dca92be-c823-4ce2-ad4f-fce6faf1471c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxcllWLL29iG",
        "outputId": "faca598c-46c7-418e-9ce4-4c84447c56c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'with',\n",
              " 'Python',\n",
              " 'and',\n",
              " 'NLTK',\n",
              " 'is',\n",
              " 'very',\n",
              " 'powerful',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 移除停用詞\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered = [w for w in tokens if w.lower() not in stop_words]"
      ],
      "metadata": {
        "id": "L3T8AbHs3KzQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20a703a4",
        "outputId": "fdbc5dc3-27f8-41db-cd13-c9c40adad05a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGeX35XI3WNA",
        "outputId": "6bf3bbb3-8f02-4c80-f1fe-540b3e918108"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural', 'Language', 'Processing', 'Python', 'NLTK', 'powerful', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xdy_4M3N3e7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"flies\", \"better\", \"studies\"]\n",
        "\n",
        "for w in words:\n",
        "    print(f\"{w:10} → Stem: {ps.stem(w):10} | Lemma: {lemmatizer.lemmatize(w)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-J7P4oF36lc",
        "outputId": "0e83e2fe-37af-448d-8001-418317205a7e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running    → Stem: run        | Lemma: running\n",
            "flies      → Stem: fli        | Lemma: fly\n",
            "better     → Stem: better     | Lemma: better\n",
            "studies    → Stem: studi      | Lemma: study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c411d26",
        "outputId": "7a6f61bf-8b2c-46f3-ba73-8794ca7d4632"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "text = \"John loves playing football with his friends.\"\n",
        "tokens = word_tokenize(text)\n",
        "tags = pos_tag(tokens)\n",
        "\n",
        "print(tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo6EPDTb40oh",
        "outputId": "70f102f2-9c31-45d4-d134-67572a91d882"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('John', 'NNP'), ('loves', 'VBZ'), ('playing', 'VBG'), ('football', 'NN'), ('with', 'IN'), ('his', 'PRP$'), ('friends', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "505a743c",
        "outputId": "fab0aef4-86a6-47fc-c25c-1c3574136b2c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "\n",
        "train = [\n",
        "    ({'love': True, 'great': True}, 'pos'),\n",
        "    ({'hate': True, 'terrible': True}, 'neg'),\n",
        "    ({'happy': True, 'good': True}, 'pos'),\n",
        "    ({'bad': True, 'sad': True}, 'neg')\n",
        "]\n",
        "\n",
        "classifier = NaiveBayesClassifier.train(train)\n",
        "print(classifier.classify({'love': True, 'amazing': True}))  # pos\n",
        "print(classifier.classify({'hate': True, 'boring': True}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrt45H865ixO",
        "outputId": "1ad9afac-9ad6-4251-b1cd-0c5c874feeb2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos\n",
            "neg\n"
          ]
        }
      ]
    }
  ]
}