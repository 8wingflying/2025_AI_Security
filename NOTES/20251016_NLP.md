## NLP

## 參考
- https://github.com/PrestonYU/Natural-Language-Processing-with-Python
- https://github.com/IKMLab/NTHU_Natural_Language_Processing
- https://web.stanford.edu/~jurafsky/slp3/ed3book_aug25.pdf

## 自然語言處理
- 自然語言處理通常包括以下步驟：
  - 文字預處理：清理和準備文字數據，例如移除特殊字元、轉換為小寫等。
  - 分詞（Tokenization）：將文字分成單字、片語或句子。
  - 詞性標註（Part-of-Speech Tagging）：為文本中的單字標註詞性，如名詞、動詞、形容詞等。
  - 命名實體識別（Named Entity Recognition）：識別文字中的命名實體，如人名、地名、日期等。
  - 句法分析（Syntax Parsing）：分析文本中的句子結構，包括主謂賓關係、子句等。
  - 情緒分析（Sentiment Analysis）：辨識文本中的情緒傾向，如正向、負向或中性。

## nltk
```PYTHON
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk

# 文本處理
text = "The quick brown fox jumps over the lazy dog."

# 斷詞
tokens = word_tokenize(text)

# 詞性標注
tagged = pos_tag(tokens)

# 命名實體識別
named_entities = ne_chunk(tagged)

# 輸出结果
print("Tokens:", tokens)
print("Tagged:", tagged)
print("Named Entities:", named_entities)
```

## 文字 -- > 向量(Vector)
- 傳統作法: one-hot encoding
- 現代做法:
  - Word2Vec	Google (Mikolov, 2013)	透過神經網路預測上下文或中心詞	訓練速度快、效果好
  - GloVe (Global Vectors)	Stanford (Pennington, 2014)	利用詞共現矩陣（co-occurrence matrix）統計全域語意	保留全局語義關係
  - FastText	Facebook (2016)	把詞拆成字詞（subword）訓練	支援未知詞 (OOV) 
- 上下文感知（contextualized）的詞嵌入模型|Contextualized Word Embedding
  - ELMo(2018)	Allen Institute (AI2)	動態（Contextualized）	透過上下文生成不同嵌入	Bi-directional LSTM
  - BERT(2018)	Google	動態（Contextualized）	使用 Transformer 雙向編碼上下文	Transformer Encoder
  - CLIP	2021	OpenAI	多模態（Text + Image）	對齊文字與影像的語意空間	Contrastive Learning (ViT + Text Transformer)
