# ğŸ§  ä½¿ç”¨ LSTM é€²è¡Œæ–‡æœ¬ç”Ÿæˆï¼ˆText Generationï¼‰

## ä¸€ã€æ¦‚å¿µèªªæ˜

**æ–‡æœ¬ç”Ÿæˆ** æ˜¯è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰ä¸­çš„ä¸€é …ç¶“å…¸ä»»å‹™ï¼Œç›®çš„æ˜¯ï¼š
> æ ¹æ“šéå»çš„æ–‡å­—åºåˆ—ï¼Œè‡ªå‹•é æ¸¬ä¸¦ç”Ÿæˆä¸‹ä¸€å€‹å­—æˆ–è©ï¼Œé€²è€Œçµ„æˆæ–°å¥å­ã€‚

### ç‚ºä½•ä½¿ç”¨ LSTMï¼Ÿ
LSTM æ˜¯ä¸€ç¨®ç‰¹æ®Šçš„ **RNNï¼ˆRecurrent Neural Networkï¼‰**ï¼Œå¯æœ‰æ•ˆè§£æ±ºï¼š
- **é•·æœŸä¾è³´å•é¡Œï¼ˆLong-term dependenciesï¼‰**
- **æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸å•é¡Œ**

å› æ­¤éå¸¸é©åˆè™•ç†æ–‡å­—åºåˆ—ç­‰é•·åºåˆ—è³‡æ–™ã€‚

---

## äºŒã€LSTM æ–‡æœ¬ç”Ÿæˆçš„æµç¨‹

| éšæ®µ | èªªæ˜ |
|------|------|
| 1ï¸âƒ£ è³‡æ–™æ”¶é›† | æº–å‚™ä¸€æ®µæ–‡æœ¬è³‡æ–™ï¼ˆä¾‹å¦‚ï¼šå°èªªã€æ–‡ç« ã€è©©è©ï¼‰ |
| 2ï¸âƒ£ å‰è™•ç† | å°‡æ–‡å­—è½‰ç‚ºæ•¸å­—åºåˆ—ï¼ˆtokenize + paddingï¼‰ |
| 3ï¸âƒ£ å»ºç«‹æ¨¡å‹ | ä½¿ç”¨ LSTM å±¤å­¸ç¿’ä¸Šä¸‹æ–‡é—œä¿‚ |
| 4ï¸âƒ£ è¨“ç·´æ¨¡å‹ | ä»¥åºåˆ—çš„å‰ n-1 å€‹å­—é æ¸¬ç¬¬ n å€‹å­— |
| 5ï¸âƒ£ ç”Ÿæˆæ–‡æœ¬ | æ ¹æ“šèµ·å§‹æ–‡å­—éè¿´ç”Ÿæˆæ–°å­— |

---

## ä¸‰ã€ç¯„ä¾‹ï¼šä½¿ç”¨ Keras å»ºç«‹ LSTM æ–‡æœ¬ç”Ÿæˆæ¨¡å‹

### ğŸ“¦ å¥—ä»¶å®‰è£
```bash
pip install tensorflow numpy pandas
```

---

### ğŸ“š ç¯„ä¾‹è³‡æ–™
```python
text = """äººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šä¸–ç•Œã€‚
ç”Ÿæˆå¼äººå·¥æ™ºæ…§è®“é›»è…¦å­¸æœƒå‰µé€ ï¼Œ
å¾æ–‡å­—ã€åœ–åƒåˆ°éŸ³æ¨‚ï¼Œçš†å¯ç”±æ¨¡å‹è‡ªå‹•ç”Ÿæˆã€‚"""
```

---

### ğŸ§¹ è³‡æ–™å‰è™•ç†
```python
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# å»ºç«‹ Tokenizer
tokenizer = Tokenizer(char_level=True)
tokenizer.fit_on_texts([text])
total_words = len(tokenizer.word_index) + 1

# ç”¢ç”Ÿè¨“ç·´åºåˆ—
sequences = []
for i in range(1, len(text)):
    seq = text[:i+1]
    token_list = tokenizer.texts_to_sequences([seq])[0]
    sequences.append(token_list)

# å°é½Šé•·åº¦
max_seq_len = max([len(x) for x in sequences])
sequences = np.array(pad_sequences(sequences, maxlen=max_seq_len, padding='pre'))

# X: å‰ n-1 å€‹å­—, y: ç¬¬ n å€‹å­—
X, y = sequences[:, :-1], sequences[:, -1]
y = to_categorical(y, num_classes=total_words)
```

---

### ğŸ§  å»ºç«‹ LSTM æ¨¡å‹
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

model = Sequential([
    Embedding(total_words, 64, input_length=max_seq_len-1),
    LSTM(128),
    Dense(total_words, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
```

---

### ğŸ‹ï¸â€â™€ï¸ æ¨¡å‹è¨“ç·´
```python
history = model.fit(X, y, epochs=200, verbose=1)
```

---

### âœ¨ æ–‡æœ¬ç”Ÿæˆå‡½å¼
```python
def generate_text(seed_text, next_chars=50):
    for _ in range(next_chars):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')
        predicted = np.argmax(model.predict(token_list, verbose=0))
        output_char = ""
        for char, index in tokenizer.word_index.items():
            if index == predicted:
                output_char = char
                break
        seed_text += output_char
    return seed_text

print(generate_text("äººå·¥æ™ºæ…§", 100))
```

---

## å››ã€é€²éšæŠ€å·§

| æŠ€è¡“ | èªªæ˜ |
|------|------|
| **å¤šå±¤ LSTM** | å¢åŠ æ¨¡å‹æ·±åº¦æå‡èªæ„ç†è§£èƒ½åŠ› |
| **Bidirectional LSTM** | æ•æ‰å‰å¾Œæ–‡èªå¢ƒ |
| **Dropout** | é˜²æ­¢éæ“¬åˆ |
| **Temperature Sampling** | åœ¨ç”Ÿæˆæ™‚æ§åˆ¶å‰µé€ æ€§ï¼ˆè¶Šé«˜è¶Šéš¨æ©Ÿï¼‰ |
| **Word-level Tokenization** | ä»¥è©ç‚ºå–®ä½ç”Ÿæˆæ›´è‡ªç„¶èªå¥ |

---

## äº”ã€ç¤ºç¯„è¼¸å‡º

è¼¸å…¥ï¼š
```
äººå·¥æ™ºæ…§
```

è¼¸å‡ºï¼š
```
äººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šä¸–ç•Œï¼Œè®“äººé¡èˆ‡æ©Ÿå™¨ä¹‹é–“çš„ç•Œç·šè¶Šä¾†è¶Šæ¨¡ç³Šã€‚
```

---

## å…­ã€å»¶ä¼¸æ‡‰ç”¨

- ğŸ“œ è©©è©ç”Ÿæˆï¼šå­¸ç¿’å¤è©©å¥æ¨¡å¼ï¼Œè‡ªå‹•å‰µä½œè©©æ­Œã€‚
- ğŸ’¬ å°è©±ç”Ÿæˆï¼šä½œç‚ºèŠå¤©æ©Ÿå™¨äººæ ¸å¿ƒæ¨¡å‹ã€‚
- ğŸ“° æ–°èçºŒå¯«ï¼šæ ¹æ“šé–‹é ­ç”Ÿæˆå®Œæ•´æ–‡ç« ã€‚
- ğŸ¨ å¤šæ¨¡æ…‹ç”Ÿæˆï¼šèˆ‡åœ–åƒã€éŸ³æ¨‚æ¨¡å‹çµåˆæˆç‚º Multimodal LLMã€‚

---

âœ… æœ¬æ•™å­¸å±•ç¤ºå¦‚ä½•ä»¥ LSTM é€²è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œå¾è³‡æ–™é è™•ç†ã€æ¨¡å‹è¨“ç·´åˆ°ç”Ÿæˆæ–°æ–‡æœ¬çš„å®Œæ•´æµç¨‹ï¼Œä¸¦å¯å»¶ä¼¸æ‡‰ç”¨æ–¼å¤šç¨®å‰µä½œå‹ä»»å‹™ã€‚

