# ğŸ¬ ä½¿ç”¨ LSTM åˆ†æ IMDB é›»å½±è©•è«–æƒ…æ„Ÿï¼ˆå« BiLSTM + Attention æ“´å……ç‰ˆï¼‰

## ğŸ“˜ ä¸€ã€å¯¦é©—ç›®æ¨™
åˆ©ç”¨ IMDB é›»å½±è©•è«–è³‡æ–™é›†ï¼Œè¨“ç·´ LSTM æ¨¡å‹ä¸¦é€²ä¸€æ­¥æ¯”è¼ƒ **LSTMã€BiLSTMã€BiLSTM + Attention** ä¸‰ç¨®æ¶æ§‹åœ¨æƒ…æ„Ÿåˆ†é¡ä»»å‹™çš„è¡¨ç¾ã€‚

---

## ğŸ“¦ äºŒã€åŒ¯å…¥å¥—ä»¶èˆ‡è¼‰å…¥è³‡æ–™
```python
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Layer
import numpy as np

# åŸºæœ¬åƒæ•¸
vocab_size = 10000
maxlen = 200
embedding_dim = 128

# è¼‰å…¥è³‡æ–™
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)

# è³‡æ–™å‰è™•ç†
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)
```

---

## ğŸ§  ä¸‰ã€LSTM æ¨¡å‹ (Baseline)
```python
model_lstm = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=maxlen),
    LSTM(128, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')
])

model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model_lstm.summary()
```

---

## ğŸ§© å››ã€BiLSTM æ¨¡å‹
```python
model_bilstm = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=maxlen),
    Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3)),
    Dense(1, activation='sigmoid')
])

model_bilstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model_bilstm.summary()
```

---

## ğŸ”¦ äº”ã€è‡ªå®šç¾© Attention æ©Ÿåˆ¶
```python
import tensorflow.keras.backend as K
from tensorflow.keras.layers import Layer

class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)
        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)
        super(Attention, self).build(input_shape)

    def call(self, inputs):
        e = K.tanh(K.dot(inputs, self.W) + self.b)
        a = K.softmax(e, axis=1)
        output = inputs * a
        return K.sum(output, axis=1)
```

---

## ğŸ§  å…­ã€BiLSTM + Attention æ¨¡å‹
```python
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input

inputs = Input(shape=(maxlen,))
embedding = Embedding(vocab_size, embedding_dim, input_length=maxlen)(inputs)

bilstm = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3))(embedding)
attention = Attention()(bilstm)
output = Dense(1, activation='sigmoid')(attention)

model_attention = Model(inputs, output)
model_attention.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model_attention.summary()
```

---

## ğŸš€ ä¸ƒã€è¨“ç·´èˆ‡è©•ä¼°
```python
history_lstm = model_lstm.fit(x_train, y_train, epochs=3, batch_size=64, validation_split=0.2, verbose=1)
history_bilstm = model_bilstm.fit(x_train, y_train, epochs=3, batch_size=64, validation_split=0.2, verbose=1)
history_attention = model_attention.fit(x_train, y_train, epochs=3, batch_size=64, validation_split=0.2, verbose=1)

# è©•ä¼°
print("LSTM æº–ç¢ºç‡:", model_lstm.evaluate(x_test, y_test, verbose=0)[1])
print("BiLSTM æº–ç¢ºç‡:", model_bilstm.evaluate(x_test, y_test, verbose=0)[1])
print("BiLSTM + Attention æº–ç¢ºç‡:", model_attention.evaluate(x_test, y_test, verbose=0)[1])
```

---

## ğŸ“Š å…«ã€ä¸‰ç¨®æ¨¡å‹æ¯”è¼ƒ
| æ¨¡å‹ | çµæ§‹ç‰¹é» | èªæ„ç†è§£ | æº–ç¢ºç‡(approx) | ç‰¹é» |
|------|-----------|------------|----------------|------|
| LSTM | å–®å‘è¨˜æ†¶åºåˆ— | ä¸­ç­‰ | 85% | çµæ§‹ç°¡å–®ï¼Œè¨“ç·´å¿«é€Ÿ |
| BiLSTM | é›™å‘åºåˆ— | è‰¯å¥½ | 90% | è€ƒæ…®ä¸Šä¸‹æ–‡èªæ„ |
| **BiLSTM + Attention** | é›™å‘ + æ³¨æ„åŠ› | **æ¥µä½³** | **92â€“94%** | èšç„¦é—œéµè©ï¼Œæé«˜å¯è§£é‡‹æ€§ |

---

## ğŸ“ˆ ä¹ã€è¨“ç·´è¦–è¦ºåŒ–
```python
import matplotlib.pyplot as plt

plt.figure(figsize=(10,4))
plt.plot(history_lstm.history['accuracy'], label='LSTM')
plt.plot(history_bilstm.history['accuracy'], label='BiLSTM')
plt.plot(history_attention.history['accuracy'], label='BiLSTM+Attention')
plt.title('æ¨¡å‹æº–ç¢ºç‡æ¯”è¼ƒ')
plt.legend()
plt.show()
```

---

## ğŸ” åã€çµè«–èˆ‡å»¶ä¼¸å»ºè­°
| å»¶ä¼¸æ–¹å‘ | èªªæ˜ |
|-----------|------|
| **Self-Attention / Transformer** | æå‡è‡³ Transformer æ¶æ§‹ï¼Œå–ä»£ RNN |
| **ä½¿ç”¨ GloVe/BERT å‘é‡** | æå‡èªæ„ç†è§£èˆ‡æ³›åŒ–èƒ½åŠ› |
| **å¯è¦–åŒ–æ³¨æ„åŠ›æ¬Šé‡** | åˆ†ææ¨¡å‹å°æ–‡å­—é—œéµå€æ®µçš„èšç„¦æ•ˆæœ |
| **å¤šå±¤ Attention** | ç–ŠåŠ æ³¨æ„åŠ›å±¤ä»¥å­¸ç¿’æ›´æ·±å±¤èªæ„æ¨¡å¼ |
