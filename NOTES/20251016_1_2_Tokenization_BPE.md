# BPEï¼ˆByte Pair Encodingï¼‰å­è©žç·¨ç¢¼
- https://zhuanlan.zhihu.com/p/424631681
- https://www.geeksforgeeks.org/nlp/byte-pair-encoding-bpe-in-nlp/
- https://ml-digest.com/byte-pair-encoding-bpe/
- https://www.youtube.com/watch?v=NrT5kmnTFCk

## ä¸€ã€BPE çš„æ ¸å¿ƒæ¦‚å¿µ
BPEï¼ˆByte Pair Encodingï¼Œå­—ç¯€å°ç·¨ç¢¼ï¼‰æ˜¯ä¸€ç¨®å¸¸ç”¨æ–¼**è‡ªç„¶èªžè¨€è™•ç†ï¼ˆNLPï¼‰**èˆ‡**å¤§åž‹èªžè¨€æ¨¡åž‹ï¼ˆLLMï¼‰**çš„**å­è©žåˆ†è©žæ³•**ã€‚å…¶ç›®æ¨™æ˜¯åœ¨ã€Œå­—è©žç´šã€èˆ‡ã€Œå­—å…ƒç´šã€ä¹‹é–“å–å¾—å¹³è¡¡ï¼Œé€éŽé »çŽ‡çµ±è¨ˆé€æ­¥åˆä½µé«˜é »å­—å…ƒå°ï¼Œå½¢æˆæ›´ç©©å®šçš„è©žå½™è¡¨ã€‚

**æ ¸å¿ƒç²¾ç¥ž**ï¼š
> å°‡èªžæ–™ä¸­çš„å­—è©žæ‹†è§£ç‚ºå­—å…ƒï¼Œç„¶å¾Œæ ¹æ“šå‡ºç¾é »çŽ‡é€æ­¥åˆä½µæœ€å¸¸è¦‹çš„å­—å…ƒå°ï¼Œå½¢æˆæ›´å¤§çš„å­è©žå–®ä½ã€‚

---

## äºŒã€BPE çš„æ¼”ç®—æ³•æ­¥é©Ÿ
å‡è¨­èªžæ–™å¦‚ä¸‹ï¼š
```
low, lower, newest, widest
```

### Step 1ï¸âƒ£ï¼šåˆå§‹åŒ–
å°‡æ‰€æœ‰å­—è©žæ‹†æˆå­—å…ƒä¸¦åŠ ä¸Šè©žå°¾ç¬¦è™Ÿ `_`ï¼š
```
l o w _
l o w e r _
n e w e s t _
w i d e s t _
```

### Step 2ï¸âƒ£ï¼šçµ±è¨ˆå­—å…ƒå°ï¼ˆpairï¼‰é »çŽ‡
| å­—å…ƒå° | å‡ºç¾æ¬¡æ•¸ |
|--------|-----------|
| (l, o) | 2 |
| (o, w) | 2 |
| (e, s) | 2 |
| (s, t) | 2 |

### Step 3ï¸âƒ£ï¼šåˆä½µæœ€é«˜é »çš„ pair
åˆä½µ (l, o) â†’ å½¢æˆ `lo`ï¼š
```
lo w _
lo w e r _
n e w e s t _
w i d e s t _
```

### Step 4ï¸âƒ£ï¼šé‡è¤‡åˆä½µç›´åˆ°è©žå½™è¡¨å¤§å°é”åˆ°ä¸Šé™
æœ€çµ‚è©žå½™å¯èƒ½åŒ…å«ï¼š
```
{l, o, w, e, r, n, d, i, s, t, lo, low, ne, new, est}
```

---

## ä¸‰ã€BPE çš„ç‰¹é»žèˆ‡å„ªå‹¢
| ç‰¹æ€§ | èªªæ˜Ž |
|------|------|
| éˆæ´»æ€§é«˜ | èƒ½åœ¨å­—èˆ‡è©žä¹‹é–“æ‰¾åˆ°å¹³è¡¡ï¼Œæ¸›å°‘æœªçŸ¥è©ž (OOV) |
| å¯å£“ç¸®è©žè¡¨ | è©žå½™è¡¨å¯æŽ§åˆ¶åœ¨ 30K~50K ç¯„åœ |
| å¤šèªžè¨€é€šç”¨ | é©ç”¨æ–¼ä¸­è‹±æ··åˆèªžè¨€ç’°å¢ƒ |
| æ¨¡åž‹å‹å¥½ | è¢« GPTã€BERTã€LLaMA ç­‰æ¨¡åž‹å»£æ³›ä½¿ç”¨ |

---

## å››ã€BPE åœ¨ NLP æ¨¡åž‹ä¸­çš„æ‡‰ç”¨
1. **OpenAI GPT ç³»åˆ—**ï¼šä½¿ç”¨ BPE å­è©žåˆ†å‰²ï¼Œè©žå½™è¡¨ç´„ 50,000 tokensã€‚
   - ç¯„ä¾‹ï¼š`unbelievable â†’ ['un', 'believ', 'able']`
2. **Hugging Face Tokenizer**ï¼š`ByteLevelBPETokenizer` è‡ªå‹•å­¸ç¿’åˆä½µè¦å‰‡ã€‚
3. **SentencePiece (Google)**ï¼šå»¶ä¼¸ç‰ˆ BPEï¼Œèƒ½ç›´æŽ¥è™•ç† byte å±¤è³‡æ–™ã€‚

---

## äº”ã€èˆ‡å…¶ä»–åˆ†è©žæ–¹æ³•æ¯”è¼ƒ
| æ–¹æ³• | å–®ä½ | å„ªé»ž | ç¼ºé»ž |
|------|------|------|------|
| Word-level | ä»¥è©žç‚ºå–®ä½ | èªžç¾©æ˜Žç¢º | è©žè¡¨é¾å¤§ï¼ŒOOV å•é¡Œåš´é‡ |
| Character-level | ä»¥å­—å…ƒç‚ºå–®ä½ | ç„¡ OOV å•é¡Œ | åºåˆ—éŽé•·ï¼Œèªžç¾©å¼± |
| BPEï¼ˆSubword-levelï¼‰ | å­è©žå–®ä½ | å¹³è¡¡é•·åº¦èˆ‡èªžç¾© | é ˆé›¢ç·šè¨“ç·´åˆä½µè¦å‰‡ |

---

## å…­ã€Python å¯¦ä½œç¯„ä¾‹
```python
from tokenizers import Tokenizer, models, trainers, pre_tokenizers

# åˆå§‹åŒ– BPE æ¨¡åž‹
tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

# è¨“ç·´è©žå½™
trainer = trainers.BpeTrainer(vocab_size=2000, show_progress=True)
tokenizer.train(["data.txt"], trainer)

# æ¸¬è©¦ç·¨ç¢¼
encoded = tokenizer.encode("unbelievable results in AI models")
print(encoded.tokens)
```

è¼¸å‡ºç¯„ä¾‹ï¼š
```
['un', 'believ', 'able', 'results', 'in', 'AI', 'models']
```

---

## ä¸ƒã€é‡é»žç¸½çµ
| é‡é»ž | èªªæ˜Ž |
|------|------|
| å®šç¾© | æ ¹æ“šå­—å…ƒé »çŽ‡é€æ­¥åˆä½µçš„å­è©žç·¨ç¢¼æ¼”ç®—æ³• |
| ç›®çš„ | é™ä½Ž OOVã€æ¸›å°‘è©žè¡¨ã€æå‡æ³›åŒ–èƒ½åŠ› |
| å…¸åž‹æ‡‰ç”¨ | GPTã€BERTã€LLaMAã€T5 ç­‰å¤§åž‹èªžè¨€æ¨¡åž‹ |
| æ ¸å¿ƒåƒ¹å€¼ | åœ¨å­—å…ƒèˆ‡è©žå½™ä¹‹é–“å–å¾—æœ€ä½³å¹³è¡¡ï¼Œä¿ç•™èªžç¾©åˆå…·é‹ç®—æ•ˆçŽ‡ |

---

> ðŸ“˜ **å»¶ä¼¸é–±è®€**ï¼š
> - Sennrich, R., Haddow, B., & Birch, A. (2016). *Neural Machine Translation of Rare Words with Subword Units*.
> - Hugging Face Tokenizers æ–‡æª”ï¼šhttps://huggingface.co/docs/tokenizers
